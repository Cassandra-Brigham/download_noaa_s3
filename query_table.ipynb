{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "from shapely import wkt\n",
    "from pathlib import Path\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf18072",
   "metadata": {},
   "source": [
    "# Make database of tile indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73336ee",
   "metadata": {},
   "source": [
    "## Get all shapefile URLs from metadata page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f06df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the tile index folder\n",
    "base_url = \"https://noaa-nos-coastal-lidar-pds.s3.amazonaws.com/laz/index.html\"\n",
    "\n",
    "# Fetch the page content\n",
    "response = requests.get(base_url)\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Failed to fetch URL: {base_url}\")\n",
    "\n",
    "#Parse content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Collect all ZIP file URLs\n",
    "tile_index_urls = []\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    if href and href.endswith('.zip'):\n",
    "        full_url = urljoin(base_url, href)\n",
    "        tile_index_urls.append(full_url)\n",
    "\n",
    "# Output the list of tile index ZIP URLs (check one or two)\n",
    "print(\"Tile index shapefile URLs:\")\n",
    "for url in tile_index_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1c27a2",
   "metadata": {},
   "source": [
    "## Make GeoDataFrame with shapefile URLs, file names and indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de4e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold individual GeoDataFrames\n",
    "gdf_list = []\n",
    "\n",
    "for url in tile_index_urls:\n",
    "    print(f\"Processing: {url}\")\n",
    "    # Download the zip file\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # Create a temporary directory to extract the zip\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            # Read the zipfile from the response content\n",
    "            with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                z.extractall(tmpdirname)\n",
    "                \n",
    "                # Identify the shapefile (.shp) within the extracted files.\n",
    "                shp_files = [os.path.join(tmpdirname, f) for f in os.listdir(tmpdirname) if f.endswith('.shp')]\n",
    "                if shp_files:\n",
    "                    shp_path = shp_files[0]\n",
    "                    # Read the shapefile into a GeoDataFrame\n",
    "                    gdf = gpd.read_file(shp_path)\n",
    "                    gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "                    \n",
    "                    # Inspect available columns (uncomment the next line to print the column names)\n",
    "                    print(gdf.columns.tolist())\n",
    "                    \n",
    "                    # Adjust the field names if necessary.\n",
    "                    if 'filename' in gdf.columns:\n",
    "                        gdf = gdf.rename(columns={\n",
    "                        'filename': 'name', \n",
    "                        'URL': 'url',\n",
    "                        'Index':'index', \n",
    "                    })\n",
    "                    else:\n",
    "                        gdf = gdf.rename(columns={\n",
    "                            'Name': 'name', \n",
    "                            'URL': 'url',\n",
    "                            'Index':'index', \n",
    "                        })\n",
    "                    \n",
    "                    # Select only the columns of interest along with the geometry.\n",
    "                    fields = ['geometry', 'name', 'url', 'index']\n",
    "                    # Check that all required fields exist in the GeoDataFrame.\n",
    "                    missing = [field for field in fields if field not in gdf.columns]\n",
    "                    if missing:\n",
    "                        print(f\"Warning: Missing expected fields {missing} in {url}\")\n",
    "                    else:\n",
    "                        gdf = gdf[fields]\n",
    "                    \n",
    "                    gdf_list.append(gdf)\n",
    "                else:\n",
    "                    print(f\"No shapefile found in the zip from {url}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {url}\")\n",
    "\n",
    "# Combine all the GeoDataFrames into one\n",
    "if gdf_list:\n",
    "    combined_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True))\n",
    "    print(\"Combined GeoDataFrame:\")\n",
    "    print(combined_gdf.head())\n",
    "else:\n",
    "    print(\"No valid GeoDataFrames were loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e638e29",
   "metadata": {},
   "source": [
    "## Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32347547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the combined GeoDataFrame\n",
    "combined_gdf_clean = combined_gdf[['url', 'name', 'geometry']].copy()\n",
    "\n",
    "# Make sure is GeoSeries\n",
    "combined_gdf_clean['geometry'] = gpd.GeoSeries(combined_gdf_clean['geometry']).to_wkt()\n",
    "\n",
    "# define output path\n",
    "output_csv = 'data_table.csv'\n",
    "\n",
    "# Save as CSV\n",
    "combined_gdf_clean.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38840c",
   "metadata": {},
   "source": [
    "# Query data table to get files of interest into smaller CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376743c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working folder\n",
    "folder = '/'\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(folder + 'data_table.csv')\n",
    "\n",
    "# Convert WKT to geometry\n",
    "df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "gdf = gdf.to_crs(\"EPSG:3857\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0b753",
   "metadata": {},
   "source": [
    "## Use spatial joins to query data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ee9b1",
   "metadata": {},
   "source": [
    "### Example: Querying all datasets within a 2 km buffer of the NOAA CUSP shorelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb83e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CUSP shoreline data at https://nsde.ngs.noaa.gov (download shapefiles for appropriate area and modify path as needed)\n",
    "coast = \"North_Atlantic\"\n",
    "coast_path = folder + \"CUSP shorelines/\" + coast + \"/\"\n",
    "shoreline_path = coast_path + coast + \".shp\"\n",
    "shoreline_gdf = gpd.read_file(shoreline_path)\n",
    "shoreline_gdf.crs  = \"EPSG:4269\"\n",
    "shoreline_gdf_m = shoreline_gdf.to_crs(\"EPSG:3857\") # convert to CRS that works in meters\n",
    "\n",
    "# Create buffer around coastlines\n",
    "coastline_buffer = shoreline_gdf_m.buffer(2000) # 2km buffer\n",
    "coastline_buffer.to_file(coast_path + coast +\"_2km_buffer.shp\") # save buffers as shapefile\n",
    "dissolved_buffer = coastline_buffer.union_all() # dissolve the buffer polygons  \n",
    "dissolved_gdf = gpd.GeoDataFrame(geometry=[dissolved_buffer], crs=coastline_buffer.crs)\n",
    "dissolved_gdf = dissolved_gdf.to_crs(\"EPSG:4326\")\n",
    "dissolved_gdf.to_file(coast_path + coast + \"_2km_buffer_dissolved.shp\") # save dissolved buffers as shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define latitude and longitude boundaries\n",
    "min_lat, max_lat = 29, 31\n",
    "min_lon, max_lon = -91.5, -87.5\n",
    "\n",
    "# Filter tiles by centroid lat/lon first\n",
    "tiles_bbox_filtered = gdf[\n",
    "    (gdf.geometry.centroid.y >= min_lat) &\n",
    "    (gdf.geometry.centroid.y <= max_lat) &\n",
    "    (gdf.geometry.centroid.x >= min_lon) &\n",
    "    (gdf.geometry.centroid.x <= max_lon)\n",
    "]\n",
    "\n",
    "print(f\"{len(tiles_bbox_filtered)} tiles after lat/lon bounding box filtering.\")\n",
    "\n",
    "# Next, apply spatial intersection with coastline buffer\n",
    "tiles_final_filtered = tiles_bbox_filtered[\n",
    "    tiles_bbox_filtered.intersects(dissolved_gdf.geometry[0])\n",
    "]\n",
    "\n",
    "print(f\"{len(tiles_final_filtered)} tiles intersect the coastal buffer after bbox filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c07e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_area_name = \"nola\"\n",
    "tiles_final_filtered.to_csv(query_area_name + '_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cac19f",
   "metadata": {},
   "source": [
    "# Use terminal and bash script to download elements in query table from s3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14183eff",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "    ./download_script.sh nola_data.csv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
